%% example.tex % Jeremy Singer % 16 Oct 12

\documentclass{mpaper}

\begin{document}

\title{Is Technical Debt Real?}
\author{Ovidiu Popoviciu}
\matricnum{2036725p}


\maketitle


\begin{abstract}
Simple abstract describing the problem, motivation, experimental design,
evaluation result and relevance.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{introduction}

\begin{enumerate}
  \item General description of the problem, motivation, relevance
  \item Contributions to state of the art 
  \item Research questions
  \item Section descriptions
\end{enumerate}

% The precise structure of an MSci paper is not mandated, but it should probably
% cover in detail the following aspects of the project. \begin{enumerate} \item
% General description of the problem, motivation, relevance \item Background
% information, possibly including a literature survey \item Description of
% approach taken to solve the problem, including high-level design and
% lower-level implementation details as appropriate \item Evaluation,
% qualitative or quantitative as appropriate \item Conclusion, including scope
% for future work \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{background}

\begin{enumerate}
  \item literature review (a bit more compressed compared to proposal)
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{methodology}

In this paper, we present an approach for aggregating development team data
sources such as project management tools, version control logs and static
analysis results to produce a timeline of technical debt and work effort over
the evolution of a software product. Our motivation is to empirically find a
correlation between technical debt and the amount of work effort involved in
development of work items. In the absence of work tracking information,
aggregation of such data sources might provide a relatively accurate estimation
of the number of hours a developer has put in.

When developing our approach, we completed the following steps:
\begin{enumerate}
  \item Designing an initial data model.
  \item Selecting data candidates (projects) that satisfy specific criteria.
  \item Collecting data from issue tracker, version control logs and generating
  static analysis output.
  \item Processing and refining data for analysis. 
\end{enumerate}

Section \ref{experimental-design} dives into the data model while section
\ref{data-selection} describes the data candidates criteria and selected
projects. The data collection process is described in Section
\ref{data-collection} while the calculation of work effort and technical debt is
described in Section \ref{data-processing}.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Experimental Design}
\label{experimental-design}

The initial step was to design a model of the aggregated data and to understand
what type of information each selected data source will provide. We consider the
following data sources relevant to the collection of data:

\begin{itemize}
  \item A \emph{Version Control System} is a system that manages changes to the
  source code. As work items are implemented, the system logs all changes made
  by the development team. The most popular version control tool is Git
  \emph{REFERENCE}. GitHub \emph{REFERENCE} and BitBucket \emph{REFERENCE} are
  hosting services for source code management.

  \item \emph{Project Management Tools} are software systems that help teams
  track, manage and collaborate on various types of units of work. A common
  example of a complex project management tool is Jira \emph{REFERENCE}. GitHub
  and BitBucket also provide an issue tracking tool with each code repository,
  although they do not provide such extensive features as Jira.
  
  \item \emph{Static Analysis Tools} are tools that analyse the source code to
  check for quality issues, security flaws and adherence to industry
  standards.There are many examples of static analysis tools: SonarQube,
  Spotbugs (formerly Findbugs), CAST, Sonargraph, etc.

\end{itemize}

All the data sources contain detailed information related to the set of changes
that the source code has suffered, the requirement that the developer is working
on and the possible quality issues she will encounter during implementation. In
many cases, these systems have been implemented by different producers and thus
are generally ``separated''. Therefore, data must be collected from each system,
in isolation. However, forms of ``light integrations'' exists for development
productivity purposes such as linking of work items to change-sets by specifying
the work item ID in the change-set description message. Such provide guidance on
how many change-sets the source code has suffered during the implementation of a
work item.

The centralised data model, presented in Figure \emph{FIGURE}, highlights the
integration of version control logs, issue tracker data and static analysis
output. The next few sections highlight the data model of each source in
isolation.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection*{Version Control}
\label{version-control}

The version control system keeps track and logs all changes that a source code
has suffered over the evolution of the product. Such logs are indispensable due
to their metadata which provides information on the accomplished work.

Git \emph{REFERENCE} is a popular version control tool that tracks changes of
the source code using branches and commits. Projects are stored in a
``repository'' which contains at least one development line, called a
``branch''. The main development branch is commonly named the \emph{master}
branch. Branches contain a stream of small units of work, called ``commits'',
which are the snapshots of the source code at a particular point in time.
Additionally, each commit contains relevant metadata with the following fields:

\begin{itemize}
  \item \emph{Object ID} - is the identifier of the snapshot;
  \item \emph{Author} - the name (or username) of the developer that committed
  the change;
  \item \emph{Message} - a short description of the change-set;
  \item \emph{Timestamp} - the time when the commit was created;
  \item \emph{Diff} - the set of changes that the source code has suffered, when
  compared to the previous commit. 
\end{itemize}

If the system allows integrations with a project management tool, changes can be
directly linked to the appropriate work unit. The most common method is to
specify the work item ID in the message field of the commit ID. As a result, it
is simpler to find out which change-sets belong to a particular work item. 

For a project managed using Git, all the metadata can be easily retrieved and
snapshots can be accessed at any time. This makes Git a powerful tool in
studying the history of changes of a software product.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection*{Project Management}
\label{project-management}

Project management tools allows developers and project managers to keep track of
the work that must be accomplished and of the issues that have been reported by
the users of the software. 

Each work item is represented in the form of a \emph{ticket}, which is a report
of the work that needs to be completed. Depending on the workflow of the team,
tickets may go through various status transitions during their lifetime which
represent checkpoints of the work involved. A ticket may contain a lot of data,
including but not limited to:

\begin{itemize}
  \item \emph{ID} - the unique identifier of the work item in the project;
  \item \emph{Type} - the type of work e.g. Feature, Bug, Enhacement, etc.;
  \item \emph{Status} - the current status of the work e.g. Created, In
  development, In testing, Closed, etc.;
  \item \emph{Summary} - a brief description of the work;
  \item \emph{Description} - a longer, more detailed description of the work
  involved;
  \item \emph{Priority} - the importance of the work as considered by the team;
  \item \emph{Assignee} - the team member responsible for completing the work;
  \item \emph{Complexity} - measurement of how complex the work item is e.g.
  Story points;
  \item \emph{Comments} - a list of comments by other team members to promote
  discussion on the work;
  \item \emph{Timestamps} - the times when ticket was created, updated, closed
  and when it is due. 
\end{itemize}

Additionally, Jira tickets contain a field called \emph{Time Tracking} that
allows tracking of the estimated time to complete the work, time logged and time
left. Moreover, for Jira, the history of changes of a work item is available
through its API in a field called \emph{Transitions}. This metadata provides a
good idea of when development for a specific work has started or ended, by
analysing the changes of the status field over time. 

Although project management tools provide relevant work tracking information,
not all tools offer the same broad scope of work data. For example, the
integrated issue tracker within GitHub and BitBucket do not offer the same
features and functionality that Jira does. Additionally, many of the fields of
tickets might be non-existent. This poses a problem to the validity of the work
effort calculations, mentioned in section \ref{data-processing}. 

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection*{Static Analysis}
\label{static-analysis}

% intro
Static analysis tools provide an elegant method of analysing source code for
possible weaknesses and quality issues without a direct execution of the entire
product. Its applicability makes it a great tool for developers to analyse their
change-sets at any time of the implementation process.

% body
The output of static analysis tools are generally weaknesses and bugs that may
lead to vulnerabilities which are considered ``lowest level'' of technical debt.
Code debt is technical debt that developers have to encounter daily
\emph{REFERENCE}. There are many static analysis tools, many of which have been
reviewed \emph{REFERENCE}, \emph{REFERENCE}. 

Although static analysis tools provide an approximate measure of the quality of
the code base, there is a danger in associating technical debt with their
output. There are multiple types of technical debt present in the software
development environment \emph{REFERENCE}. For example, static analysis tools
cannot predict that the requirements gathering phase was not completed
appropriately and thus it might take more time for a team member to understand
the expectations of a work item, thus indirectly increasing effort. However, as
a simplifying assumption, in this paper we only consider code debt in the
calculation of a technical debt index. 

The output of a static analysis tool that we are interested in contains the
following fields:

\begin{itemize}
  \item \emph{ID} - a unique identifier of the quality issue;
  \item \emph{Priority} - a generic measure of severeness e.g. High, Medium,
  Low;
  \item \emph{Description} - a short description of the code violation;
  \item \emph{Location} - the location of the quality issue, represented by the
  file name and line number. 
\end{itemize}

This type of output gives a representation of the possible low-level weaknesses
that developers might introduce and encounter during the implementation process.
Additionally, by using the version control logs it is possible to track the
items that have been introduced or removed for each change-set as well as the
total amount of issues that a team member had to deal with. 

Unfortunately, not all projects use a static analysis tool to check for issues
and, if some do, they are not publicly available in a centralised system as well
as not being available for each revision of the source code. This challenge was
addressed and is described in section \ref{data-collection}. 

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Candidates selection}
\label{data-selection}

% intro
For the purpose of this experiment, we had to apply the data collection
processing phases on a set of software projects. These projects had to satisfy a
certain criteria to be included:

\begin{itemize}
  \item Use \emph{Git} as a version control tool due to its powerful features,
  applicability and popularity. Additionally, the project must be open-source to
  access the history of snapshots.
  \item Use a project management tool that is publicly accessible to retrieve
  work item data.
  \item Use \emph{Java} as the main programming language. This was a simplifying
  criteria to keep the data collection and processing consistent. Additionally,
  this aided our decision in selecting a single static analysis tool.
  \item The project should have a single main development branch, preferably the
  ``master'' branch. This relieves the data collection process of analysing
  multiple branches.
  \item The version control hosting system should integrate with the project
  management tool to highlight the commits that belong to a specific work item.
  \item The project uses Maven as its build system due to simplifications in
  development process of the data collection tool. Multiple build systems could
  be added if necessary.
\end{itemize}

% body
Table \emph{TABLE} shows the selected open-source projects that have their data
collected and analysed as well as their size (LOC), version control and project
management tools. Out of the ten projects, we have only included five for
analysis of results due to an incomplete data collection process. This was a
result of:

\begin{itemize}
  \item Unavailable APIs for accessing project management tool.
  \item Non-existent static analysis output. This might be due to a ``Zero
  Bugs'' policy set by the development team.
  \item Failing build process for the majority of revisions. The build process
  was failing on too many revisions to provide any meaningful data.
\end{itemize}

Since the projects selected for this experiment are managed by different
development teams, some of the data sources are distributed and use different
underlying technologies. Additionally, not all projects make use of a static
analysis tool and therefore such data might not be publicly available. As a
result, we have decided to execute static analysis on each revision of the main
branch our candidate projects. This brings the advantage of a consistent data
model of quality issues and fills in the void left by the lack of static
analysis data. However, this decision introduced an additional overhead in the
data collection process.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Data collection}
\label{data-collection}

% intro
To collect the selected data from multiple systems we built a tool that
retrieves data from each source and centralises them into a single database for
processing and analysis. Additionally, this method allows us to reproduce the
experiment without any outside changes to the dataset while future work items do
not incur an additional data collection overhead. Moreover, due to the choice of
database technology, new data could be added from additional projects without
any changes to the initial data model. Figure \emph{FIGURE} shows a simple view
of the data model.

The tool has been built using Java and open-source technologies, and is
available to use by anyone (MIT License) \emph{REFERENCE}. The tool uses the
following technologies for collecting data:

\begin{itemize}
  \item jGit \emph{REFERENCE} - is a Java library for interacting with the Git
  version control system;
  \item REST Java Client for Jira \emph{REFERENCE} - is a Java library that
  provides a simple interaction with Jira APIs;
  \item GitHub API for Java \emph{REFERENCE} - is a Java library that provides
  integration with GitHub API for retrieving issues;
  \item MongoDB \emph{REFERENCE} - NoSQL database technology for storing
  collected data;
  \item Spotbugs \emph{REFERENCE} - tool for identifying bug patterns in source
  code;
  \item Other technologies - Spring Boot \emph{REFERENCE}, Angular \emph{REFERENCE} and ChartJS \emph{REFERENCE}
  for visualising data.
\end{itemize}

The data collection process is shown in figure \emph{FIGURE}. Firstly, selected
project repositories are cloned to the local storage. Secondly, the tool starts
the ``collection step'', where each revision of the project history is processed
serially, starting with the latest revision and going back in time. The
collection step is composed of multiple sub-steps:
\begin{enumerate}
  \item \emph{Retrieve commit metadata}.
  \item \emph{Search and retrieve ticket(s)}. The current commit message is
  analysed for mentions of ticket IDs. For any ticket ID found, the tool
  retrieves the ticket from the project management tool API.
  \item \emph{Build current revision}. This is a preparation step for the static
  analysis. The source code of the project must be compiled into classes and
  packaged into JARs for static analysis to complete.
  \item \emph{Statically analyse the JARs}. Scan the current revision JARs for
  quality issues. If the build process fails, then the static analysis scan is
  skipped, due to missing JAR files. 
\end{enumerate}

Data is stored at the end of each substep to avoid an inconsistent commit stream
in case one of the next one fails or the tool crashes.

During development, the building and static analysis steps incurred an extreme
overhead that was slowing down the data collection process. For a repository
with thousands of revisions, there would have been a significant time overhead
that would overstep the timeline of the experiment. Therefore, the build process
was customised by skipping a few unnecessary tasks that we did not consider in
scope for this experiment:

\begin{itemize}
  \item Generating of documentation;
  \item Compiling and running tests;
  \item Test coverage checks;
  \item Integrated code analysis.
\end{itemize}

Some projects had an integrated static analysis task as part of the build
process. We decided to skip it due to the implementation of the Spotbugs
analysis step and due to complexities in the integration with multiple analysis
tools. 

Additionally, the tasks that compile and run the testing suites were ignored.
Firstly, testing was considered a separate activity to the implementation of new
features, fixing bugs and adding enhacements. Thus, in this case, code debt was
split into ``implementation debt'' and ``testing debt''. Testing debt refers to
shortcuts taken during the testing phases \emph{REFERENCE}. Secondly, the
testing task added an additional overhead to the build process. In reality,
testing is a fundamental part of development and is tightly integrated with
implementation of work items, especially in the practice of Test Driven
Development. It is one of the top most mentioned type of technical debt in a
mapping study by X et. al \emph{REFERENCE}, along with code, architecture and
design debt. The integration of testing debt within this experiment is
considered as a future work item, mentioned in section \ref{future-work}.

% conclusion
Although only \emph{N} projects have been selected out of the initial 11
presented in section \ref{data-selection}, using our data collection process we
have managed to retrieve and analyse \emph{M} commits and \emph{X} issues along
with an average of \emph{T} unique quality issues. Additionally, this process
was repeated a second time due to changes in the static analysis collection
step. Section \ref{results} dives into the results of our analysis. 

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Data processing}
\label{data-processing}

% intro
Although, the collection step gathered data from three systems and stored them
into a single source of truth, in its current state it could not have been used
to derive any meaningful information on the correlation between technical debt
and work effort. Therefore, a ``processing stage'' was implemented in the tool,
that allows the calculation of these metrics and visually displays statistics on
using charts.

% body
The processing stage revolved around the concept of work items (WI). Work items
are pieces of work assigned to developers of the project. Each item is composed
of a ticket (T) and a list of commits (C), ordered chronologically. Tickets
represents the pieces of data retrieved from the project management tool. The
list of commits represent a subset of the total revisions of the repository that
belong to that specific work item. As mentioned in section
\ref{data-collection}, commits are assumed to belong to a specific ticket if
their commit message actively references the ticket ID of the project management
tool.

The work items can be deduced from the data collected. However, there are two
fundamental measures which must be derived:
\begin{itemize}
  \item Work effort (WE) - the total number of man-hours put in for a specific
  work item;
  \item Technical Debt (TD) - the total number of technical debt items that have
  affected the work at the time of implementation.
\end{itemize}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection*{Work Effort}
\label{work-effort}

% calculation of work effort
As mentioned, the work effort is the total number of hours put in by a single
developer towards the design and implementation of a work item. For simplicity,
we considered that an item of work followed a linear pattern and that a
developer was working on a single work item at a time.

From this assumption, we considered that each work item has a start time ($t1$)
and end time ($t2$), both expressed as timestamps in Greenwich Mean Time (GMT)
format. A simple method for calculating the work effort is to take the
difference between these two timestamps. This makes it easy to calculate the
working hours on the same day:

\begin{equation}
  \label{eq-work-effort-simple}
  WE = hours(t1 - t2)
\end{equation}

For example, if a developer starts work on Monday at 9:00 GMT and finishes work
at 15:00 GMT, then the total of effort put in is 6 hours. However, a work item
might span multiple days and thus this solution would include hours outside
hours. To cover for this case, we introduced another simplyfing assumption.
Assume that the normal working hours per day would not exceed 8 hours. This
means that work spread over multiple days would be calculated as follows:

\begin{equation}
  \label{eq-work-effort-days}
  WE = (days(t1 - t2) + 1) * 8
\end{equation}

If the work spans multiple days, then the total effort is the total number of
days multiplied by the number of working hours per day. For example, let's
assume that a developer starts a work item on Monday, 9:00 GMT and finishes his
work on Tuesday, 21:00 GMT. This equation will yield 16 hours of work spanning
two days (Monday and Tuesday). We recognize that there are multiple factors that
affect the work effort put in for a work item which are not modelled as part of
the calculation. These two equations were chosen for their simplicity and to
match the type of data at disposal. 

From the data available, there are two sets of timestamps that could be used for
work effort calculation: commit and ticket timestamps. Metadata of the project
management tool might provide an accurate estimation of the times an engineer
has started and finished his work. Additionally, version control logs provide
additional metadata of each commit, including the time where a commit was pushed
to the repository. Moreover, these two ``types'' of timestamps may be combined.
For example, the start time can be derived from the ticket creation timestamp
while the completion time can be derived from the last commit timestamp that
belongs to a specific work item.

% a. by ticket
The ticket is a running ``report'' of the work involved. In the issue tracker,
it is a reference point for the assignee and other members of the team. As
mentioned in section \ref{experimental-design}, the ticket contains various
fields and metadata. The most of relevant fields for calculating work effort are:

\begin{itemize}
  \item Creation timestamp - the timestamp when the ticket was created and added
  to the project management tool;
  \item Closure timestamp - the timestamp when the ticket was marked as resolved
  or closed.  
\end{itemize}

The difference between these two timestamps could give an accurate estimation of
the number of hours, calculated using equations \ref{eq-work-effort-simple} and
\ref{eq-work-effort-days} mentioned above. However, they might not reflect the
work hours only. For example, there might be an additional time between the
ticket creation and the work start timestamps. For example, a ticket may have
been created for a week before the start of a new sprint, should the team employ
an agile development method. Moreover, the team might employ code review
practices meaning that the time taken to review one's work is included in the
calculated work effort. 

Fortunately, Jira contains an additional metadata field, called
\emph{Transitions} that lists the history of changes that the ticket has
suffered. Status changes are changes in the workflow of the ticket, such as from
\emph{Open} to \emph{In Progress} and from \emph{In Progress} to \emph{In
Review}. The timestamps of these transitions give a more accurate time range of
development work hours. GitHub and BitBucket do not have the transition feature
build-in.

As a result, the selected timestamps for work start ($t_{1}$) and end work
($t_{2}$) based on ticket metadata are chosen as follows:
\begin{equation}
  \label{eq-ticket-start}
  \begin{aligned}
    t_{1} = Timestamp(Transition(\textrm{Open to In Progress})) \\ 
      \textrm{or} \quad Timestamp(\textrm{Created}) \\
  \end{aligned}
\end{equation}

\begin{equation}
  \label{eq-ticket-closed}
  \begin{aligned}
    t_{2} = Timestamp(Transition(\textrm{In Progress to Closed})) \\ 
      \textrm{or} \quad Timestamp(\textrm{Closed})    
  \end{aligned}
\end{equation}

% b. by commit
An alternative method of selecting work timestamps is from the version control
logs collected. The timestamp of a revision flags the date and time when a set
of changes have been implemented. Since a work item is composed of one or
multiple revisions, a relatively accurate estimation of work effort could be
derived.

As presented in figure \emph{REFERENCE}, a simple method of calculation is to
take the difference between the last and first commit timestamps. Using this
method, the last commit timestamp reflects the time of the the last changes
pushed by the developer. However, the first commit timestamp does not reflect
the date and time when work has started. This is because the timestamp of
commits represent the time when the changes have already been implemented. Thus,
the results of the calculation lacks the effort accomplished before the first
commit has been pushed.

To cover for this case, we assumed that once a contributor finished a work item
she would get started on the next. Figure \emph{REFERENCE} visualises the case.
We considered that the missing work effort is measured as the number of hours
between the last commit timestamp of a previous work item and the first commit
timestamp of the current work item. However, a developer may not have a previous
contribution to the project. In this case, the number of hours is calculated
from the first commit timestamp of the work item.

The equations for the two timestamps are as follows:
\begin{equation}
  \label{eq-commit-start}
  \begin{aligned}
    t_{1} = Timestamp(C_{-1}) \quad or \quad Timestamp(C_{0})
  \end{aligned}
\end{equation}

\begin{equation}
  \label{eq-commit-end}
  \begin{aligned}
    t_{2} = Timestamp(C_{n})
  \end{aligned}
\end{equation}

where $C_{i}$ represents a commit from the initial work item if $i = 1,...n$.
$C_{-1}$ represents the last commit of the previous work item.

Measuring work effort accurately from limited data is a challenging task. Most
open-source projects rely on contributions from software professionals all over
the globe. In an ideal case, developers would measure their work time on the
project management tool or on an internal worksheet. However, 54\% of software
professionals working remotely do not track their work effort
\cite{DeveloperSurvey2018}.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection*{Technical Debt}

% calculation of technical debt
% total, added, removed, locality

% conclusion

Factors such as technical experience, knowledge of system, familiarity with
libraries and dependencies used, personality and intrinsic complex details of
the work involved and have not been considered in the calculation. Open-source
projects are generally maintained by international teams consisting of
developers with various skillsets. Any international developer may contribute to
open-source projects. Collecting such data means tracking down and interviewing
each collaborator. Moreover, there is a challenge in modelling subjective
factors within such calculations.

This stage was implemented as a server running a REST API, where each request is
specific to the project and to the calculation required. This allowed the server
to process the data and compute work effort and technical debt for a single
project in isolation, as requested. The results are displayed on a Single Page
Application (SPA) dashboard \emph{REFERENCE}. The calculation processes becomes
computationally expensive as the size of the repository data grows (number of
issues and commits). For this purpose, appropriate caching optimisations have
been implemented to speed up retrieval of results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{results}

Provide the results of the data processing step. Aggregate the data into tables
based on the repositories chosen and provide a couple of example graphs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{discussion}

Discuss the results and limitations of the experiment.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Threats to validity}
\label{validity}

Risks to the methods of extraction, calculation and aggregation of technical
debt, work effort, ticket complexity, etc.

\begin{itemize}
  \item Work - unrealistic expectation for developers to work 8 hours per day
  for open source projects.
  \item Work - did not exclude weekends, time off, etc.
  \item Commits - rebase modifies timestamps
\end{itemize}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Future Work}
\label{future-work}

Describe the items that could be improved and expand on the next steps within
this area of research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\bf Acknowledgments.} This is optional; it is a location for you to thank
people

\bibliographystyle{abbrv}
\bibliography{mdiss}

\end{document}